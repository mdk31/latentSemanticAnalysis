{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba7cf0f",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis\n",
    "\n",
    "One natural language processing (NLP) method I've always found interesting is latent semantic analysis (LSA), an early  method that uses matrix decomposition to discover unobserved \"latent\" semantic associations between words in different documents. In this project, I want to use this method to construct an index for a particular work. This was inspired by Cosma Shalizi's data analysis book, in which he discusses using latent semantic indexing (LSI) to construct an index in Adam Smith's \"The Wealth of Nations.\" First, we import the modules we will need. The `gensim` library contains import modules for topic modelling, including the `LsiModel`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1d5826fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from gensim.models import LsiModel\n",
    "from gensim import models\n",
    "from gensim import corpora\n",
    "from gensim.parsing.preprocessing import remove_stopwords, stem_text\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "import pandas as pd\n",
    "from gensim import similarities\n",
    "import logging\n",
    "logger = logging.getLogger(\"PyPDF2\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897385a",
   "metadata": {},
   "source": [
    "Next, we read in the pdf data and collect the document in a list called `cor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "22cbc176",
   "metadata": {},
   "outputs": [],
   "source": [
    "wealth_file = 'wealthofnations2.pdf'\n",
    "wealth = open(wealth_file, 'rb')\n",
    "wealthReader = PyPDF2.PdfReader(wealth)\n",
    "\n",
    "# The actual text starts on page 109, which is 111 of the pdf (which is 110 in Python as lists are indexed from 0)\n",
    "first_page = 7\n",
    "last_page = 784\n",
    "\n",
    "# Collect the corpus\n",
    "cor = []\n",
    "for num in range(first_page, last_page):\n",
    "    cor.append(wealthReader.pages[num].extract_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17193c02",
   "metadata": {},
   "source": [
    "Then we construct a generator function that strips out numeric values, stems the words, and then tokenizes each document (page of the text). Once each document is processed, we construt a mapping between tokens and their integer IDs with `corpora.Dictionary` that takes an iterable. Once we have the dictionary, we filter extreme values, removing ... Finally, we construct a document-term matrix that creates a list of `(token_id, token_count)` tuples for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1a4619e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    for document in cor:\n",
    "        doc = remove_stopwords(strip_numeric(stem_text(document)))\n",
    "        yield gensim.utils.tokenize(doc, lower=True)\n",
    "        \n",
    "texts = preprocessing()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=1)\n",
    "doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in preprocessing()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ffde6",
   "metadata": {},
   "source": [
    "## Latent Indexing\n",
    "\n",
    "How would we normally construct an index? One obvious way to do this is to look at raw word counts and peel off the top $n$ pages that most frequently mention a particular word. In this case, our word of interest is \"agriculture.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ed77a551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printed page 166 has the word 'agriculture' 4 times\n",
      "Printed page 126 has the word 'agriculture' 3 times\n",
      "Printed page 293 has the word 'agriculture' 3 times\n",
      "Printed page 300 has the word 'agriculture' 3 times\n",
      "Printed page 333 has the word 'agriculture' 3 times\n",
      "Printed page 6 has the word 'agriculture' 2 times\n",
      "Printed page 121 has the word 'agriculture' 2 times\n",
      "Printed page 154 has the word 'agriculture' 2 times\n",
      "Printed page 551 has the word 'agriculture' 2 times\n",
      "Printed page 761 has the word 'agriculture' 2 times\n"
     ]
    }
   ],
   "source": [
    "query = 'agriculture'\n",
    "query_id = dictionary.token2id.get(query)\n",
    "doc_frequencies = [(doc_id, sum(count for tok_id, count in doc if tok_id == query_id)) for doc_id, doc in enumerate(doc_term_matrix)]\n",
    "doc_frequencies.sort(key =lambda x: x[1], reverse=True)\n",
    "doc_frequencies = doc_frequencies[:10]\n",
    "\n",
    "for doc_id, count in doc_frequencies:\n",
    "    print(f\"Printed page {doc_id + 2} has the word '{query}' {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5670ec1c",
   "metadata": {},
   "source": [
    "This doesn't work in general, because it will only include pages that have the word \"agriculture\" and will not include pages that discuss topics related to agriculture but don't use the word itself (Shalizi 2016, p. 383). Instead, we will construct a word-document matrix that we can use to find latent relationships between words and documents in a corpus. The idea is that we can discover these relationships and use them to find documents that are most related to a particular word. \n",
    "\n",
    "In the original LSI paper, the authors use a pure \"count\" matrix that measures the number of times each word appears in each document (which in this project is a page of the text). Instead, we  use a term frequency-inverse document frequency (tf-idf) matrix, which in addition to counts incorporates how frequently a word appears across the corpus. We compute the tf-idf matrix using our document-term matrix with `TfidfModel`, which computes the tf-idf weights for each token. We then transform the full corpus using our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac1cae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "be4442de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(doc_term_matrix)\n",
    "corpus_tfidf = tfidf[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214fcc8",
   "metadata": {},
   "source": [
    "For an $m \\times n$ matrix $A$, we can decompose $A$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "A = U \\Sigma V^T\n",
    "\\end{equation}\n",
    "\n",
    "where $U$ and $V$ are orthogonal matrices and $\\Sigma$ is a diagonal matrix. The columns of $U$ are the left singular vectors of $A$ and are the eigenvectors of $AA^T$. Similarly, the columns are the right singular vectors of $A$, or the eigenvectors of $A^TA$. These two matrices can be used to eigendecompose the matrices $AA^T$ or $A^TA$. The idea behind LSI is that, once we have the SVD decomposition of $X$, we can find a lower dimensional representation of the matrix by truncating the singular values to only keep the $k$ largest ones. It is a well-known theorem that this produces the best rank-$k$ approximation to the original matrix (measured in terms of the Frobenius norm). \n",
    "\n",
    "We rarely have a clear idea of what $k$ should be in any practical application. As with principal component analysis, this is primarily a trial and error process. The original LSI paper recommends using between 50-100 factors (Deerwester et. al. 1990, p. 7). In this case, we use 50 singular vectors to truncate the SVD. The module `sklearn` has a fast truncated SVD decomposition in the `TruncatedSVD` class.\n",
    "\n",
    "Below, we instantiate the `LsiModel` with 50 topics (the latent factors we discussed in the previous paragraph). Then we take the query word (or \"pseudo-document\" in the language of (Deerwester et. al. 1990)) and transform it into the $k$-dimensional space of latent concepts. Once we have done that, we calculate the similarity between every document to every other document, represented in this space. Finally, we can see which documents are most similar to our query. We print the top 20 most similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2bac44de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.557081 298\n",
      "0.541889 332\n",
      "0.5403838 331\n",
      "0.5300852 302\n",
      "0.5190811 320\n",
      "0.514124 319\n",
      "0.4811794 164\n",
      "0.47760132 321\n",
      "0.47642392 291\n",
      "0.46523505 289\n",
      "0.4324791 290\n",
      "0.42376462 322\n",
      "0.41840863 544\n",
      "0.3964745 760\n",
      "0.39538434 5\n",
      "0.39408797 548\n",
      "0.39388704 551\n",
      "0.39342406 273\n",
      "0.38387808 300\n",
      "0.37645245 312\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=50)\n",
    "vec_bow = dictionary.doc2bow(query.lower().split())\n",
    "\n",
    "vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
    "index = similarities.MatrixSimilarity(lsi[doc_term_matrix])\n",
    "unsorted_similarity = index[vec_lsi]\n",
    "sorted_similarity = sorted(enumerate(unsorted_similarity), key=lambda x: x[1], reverse=True)\n",
    "for index, similarity in sorted_similarity[:20]:\n",
    "    print(similarity, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bd1df",
   "metadata": {},
   "source": [
    "Let's look at the most similar documents to our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8f11a773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'340The Wealth of Nations\\nThe foreign commerce of Spain and Portual to the other parts\\nof Europe, though chiefly carried on in foreign ships, is very con-\\nsiderable. That to their colonies is carried on in their own, and is\\nmuch greater, on account of the great riches and extent of those\\ncolonies. But it has never introduced any considerable manufac-\\ntures for distant sale into either of those countries, and the greater\\npart of both still remains uncultivated. The foreign commerce of\\nPortugal is of older standing than that of any great country in\\nEurope, except Italy.\\nItaly is the only great country of Europe which seems to have\\nbeen cultivated and improved in every part, by means of foreign\\ncommerce and manufactures for distant sale. Before the invasion\\nof Charles VIII., Italy, according to Guicciardini, was cultivated\\nnot less in the most mountainous and barren parts of the country,\\nthan in the plainest and most fertile. The advantageous situation\\nof the country, and the great number of independent status which\\nat that time subsisted in it, probably contributed not a little to this\\ngeneral cultivation. It is not impossible, too, notwithstanding this\\ngeneral expression of one of the most judicious and reserved of\\nmodern historians, that Italy was not at that time better cultivated\\nthan England is at present.\\nThe capital, however, that is acquired to any country by com-\\nmerce and manufactures, is always a very precarious and uncer-tain possession, till some part of it has been secured and realized\\nin the cultivation and improvement of its lands. A merchant, it\\nhas been said very properly, is not necessarily the citizen of any\\nparticular country. It is in a great measure indifferent to him from\\nwhat place he carries on his trade; and a very trifling disgust will\\nmake him remove his capital, and, together with it, all the indus-\\ntry which it supports, from one country to another. No part of it\\ncan be said to belong to any particular country, till it has been\\nspread, as it were, over the face of that country, either in buildings,\\nor in the lasting improvement of lands. No vestige now remains of\\nthe great wealth said to have been possessed by the greater part of\\nthe Hanse Towns, except in the obscure histories of the thirteenth\\nand fourteenth centuries. It is even uncertain where some of them\\nwere situated, or to what towns in Europe the Latin names given\\nto some of them belong. But though the misfortunes of Italy, in\\nthe end of the fifteenth and beginning of the sixteenth centuries,\\ngreatly diminished the commerce and manufactures of the cities\\nof Lombardy and T uscany, those countries still continue to be\\namong the most populous and best cultivated in Europe. The civil\\nwars of Flanders, and the Spanish government which succeeded\\nthem, chased away the great commerce of Antwerp, Ghent, and\\nBruges. But Flanders still continues to be one of the richest, best\\ncultivated, and most populous provinces of Europe. The ordinary'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor[332]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2abe2a9",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990). Indexing by latent semantic analysis. Journal of the American society for information science, 41(6), 391-407.\n",
    "- Smith, A. (1970). The Wealth of Nations Books Iâ€”III. Pelican Books.\n",
    "- Shalizi, C. R. (2016). Advanced data analysis from an elementary point of view. 2013. URL http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
